{"meta":{"title":"BuT的博客","subtitle":"不要因为路远而不走 只有走就能走得到 不要因为石重而不搬 只要搬就能搬得动","description":"","author":"BuT","url":"http://bigtrex.top","root":"/"},"pages":[{"title":"分类","text":"","path":"categories/index.html","date":"07-27","excerpt":""},{"title":"标签","text":"","path":"tags/index.html","date":"07-28","excerpt":""},{"title":"","text":"type: “index”","path":"index/index.html","date":"09-27","excerpt":""},{"title":"index","text":"","path":"index/index-1.html","date":"09-27","excerpt":""}],"posts":[{"title":"10.5 木兰草原一日","text":"10.5 木兰草原一日双人套票-云中战歌+萌心牧场+飞天魔毯+游乐项目9选4 木兰草原国庆演出时间表 plan A第一站-吉祥广场 《敖包祭祀》- 11:30 第二站-云中镇古街 《黄梅戏》- 14:30 17:40（backup！） 《王府招婿》- 15:00 18:00（backup！） 《木兰将军巡游》- 15:30 18:30（backup！） 第三站-云中镇剧场 《花木兰·云中战歌》- 16:00-16:45 19:00-19:45（backup！） 第四站-白鹭湖 《木兰情》- 18:50 第五站-梦栖谷·凤凰广场 《篝火晚会》- 20:10 p.s 提早去有好位置 《音乐烟花秀》- 20:30 maybe之后会有篝火蹦野迪和泼水节&#9835; 游玩项目 激流勇进（地上有水的话不能站人） 彩虹滑道（和滑草比较像 :worried: ） 滑草（草道上有水很臭！:worried:） 水上自行车（凉快！） 凌波飞度（吊着绳索过河） 射箭（一人六只箭） 竹筏（据说可） 吃奶鱼 悠波球 演出间隔就闲逛找点乐子！ WARNING! 带！花！露！水！ 带！伞！ plan Blie still and sleep！&#10084;:two_hearts: Hotel 草原可以租帐篷露营 hotel待定 Eating可以带零食野餐，景区maybe也有吃饭的地方。 TrafficTaxi or 木兰草原","path":"小博专属/10_5_木兰草原/","date":"09-26","excerpt":"","tags":[]},{"title":"scrapy自查","text":"scrapy自查1. scrapy安装 2. scrapy命令 scrapy安装123456# windowspip install wheel[下载twisted](https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted)pip install Twisted-20.3.0-cp39-cp39-win_amd64.whlpip install pywin32pip install scrapy scrapy命令创建工程：scrapy startproject name_0 在工程文件夹下创建爬虫文件（首先进入该文件夹）：scrapy genspider name_1 www.balabala.com 执行程序：scrapy crawl name_1","path":"scrapy自查/","date":"08-20","excerpt":"","tags":[]},{"title":"selenium自查","text":"selenium自查1. 无头浏览器 2. 反selenium检测 3. selenium截取全屏 4. selenium不自动关闭浏览器 5. selenium切换到iframe 6. selenium窗口最大化 无头浏览器无头浏览器即执行爬虫程序时浏览器在后台运行而不显示 123456789101112from selenium import webdriverfrom selenium.webdriver.edge.options import Options# 设置无头edge_options.add_argument(&#x27;--headless&#x27;)# 禁用GPU，防止无头模式出现莫名的BUGedge_options.add_argument(&#x27;--disable-gpu&#x27;)# 将参数传给浏览器edge = webdriver.Edge(options=edge_options) 反selenium检测防止对应网站针对selenium进行反爬 123456789101112from selenium import webdriverfrom selenium.webdriver.edge.options import Options# 开启开发者模式edge_options.add_experimental_option(&#x27;excludeSwitches&#x27;, [&#x27;enable-automation&#x27;])# 禁用启用Blink运行时的功能edge_options.add_argument(&#x27;--disable-blink-features=AutomationControlled&#x27;)# 将参数传给浏览器edge = webdriver.Edge(options=edge_options) selenium截取全屏使用selenium进行模拟登录时，如果网页存在验证码，那么是无法再通过requests对验证码的url发起get请求下载图片的，这就相当于用两个不同的浏览器进行登录，验证码肯定是不相同的 那么只能对响浏览器进行截屏，接着定位到验证码图片的矩形区域以截取验证码图片 我们当然希望通过程序自动截屏并且定位验证码图片的坐标，所以一般思路是获取验证码图片对应的标签，然后获取对应的坐标和长宽数据 但是有一个问题是，如果页面无法全部显示在当前浏览器界面时，使用selenium截屏只能截取部分页面，也就是电脑显示的部分，而标签定位是相对于整张页面的，所以在部分页面截屏上无法准确定位到验证码图片坐标 那么必须先通过selenium截取全部页面 当然我们也可以借助其它工具在部分截屏页面中定位到验证码图片的坐标，但是一般都需要手动操作，那模拟登录还有什么意义。。。 截取整张页面的一般方法 最常见的就是先手动设置浏览器或者屏幕分辨率，使得整张页面可以在浏览器中显示，此时的截屏当然是整张页面了，这样做比较麻烦 通过js代码设置浏览器分辨率，但是此时只能使用无头浏览器 123456width = edge.execute_script(&quot;return document.documentElement.scrollWidth&quot;)height = edge.execute_script(&quot;return document.documentElement.scrollHeight&quot;)# 将浏览器的宽高设置成页面的宽高edge.set_window_size(width, height) ps.使用selenium进行图像处理时一定要用png格式，jpg格式会存在解码错误 selenium不自动关闭浏览器123456789from selenium import webdriverfrom selenium.webdriver.edge.options import Optionsedge_options.add_experimental_option(&quot;detach&quot;, True)# 将参数传给浏览器edge = webdriver.Edge(options=edge_options) 如果需要关闭可以加入edge.quit() selenium切换到iframe 如果iframe具有id或name属性，直接通过edge.switch_to.frame(&quot;id\\name&quot;) 其中id是唯一的，name可能不唯一 如果iframe不具有id和name，那么首先利用一般定位方式定位到iframe标签，再切换到iframe 1234iframe=edge.find_element()edge.switch_to.frame(iframe) selenium窗口最大化1edge.maximize_window()","path":"selenium自查/","date":"08-18","excerpt":"","tags":[]},{"title":"bs4方法自查","text":"bs4库方法自查Beautiful Soup库是解析、遍历、维护“标签树”的功能库 BeautifulSoup使用方法123456from bs4 import BeautifulSoupimport bs4soup=BeautifulSoup(&lt;html&gt;response.text&lt;/html&gt;,&#x27;html.parser&#x27;)soup=BeautifulSoup(open(&#x27;*.html&#x27;,&#x27;r&#x27;),&#x27;html.parser&#x27;) BeautifulSoup类基本元素 基本元素 含义 Tag 标签，以&lt;&gt;…&lt;&#x2F;&gt;组成 Name 标签名，&lt;p&gt;…&lt;&#x2F;p&gt;的名字是p：&lt;tag&gt;.name Attributes 标签的属性，字典形式组织：.attrs NavigableString 标签内非属性字符串，标签下只能有一个子节点：.string，返回NavigableString text 标签内字符串，可跨域层级：.text，返回str Comment 标签内字符串的注释部分：.string text和string的区别 BeautifulSoup类常用方法一篇总结bs4非常相近且准确的blog bs4方法详情","path":"bs4方法自查/","date":"08-05","excerpt":"","tags":[]},{"title":"关于MD锚点的坑","text":"关于MD锚点的坑锚点是一个方便定位的东西，用起来还是很香的 但是不同MarkDown对锚点的解释都有些许区别，导致不同平台上锚点可能会失效 MarkDown中对于锚点的语法定义如下： 12345[描述](#定位的title)...# 定位的Title 多级标题都可以作为’定位的title’，另外标题中大写字母会自动转为小写字母 For Example … … … … … For Example这样的锚点在vscode这样的编辑器上是没有问题的，但是放到如github，csdn上会失效 代码如下： 123[For Example](#for-example)## For Example 对于hexo+github搭建的博客，这样的锚点失效在于#后的内容和标题不一致，主要在于英文大小写，中文不存在如上问题 修改为如下的代码，在hexo+github上可以实现锚点，然而对于vscode这样的编辑器当然失效了，另外这样的锚点在csdn上也会失效 123[For Example2](#For-Example2)## For Example2 For Example2 … … … … … For Example2目前来看纯MarkDown语法的锚点总会存在一些坑，一个通用解决方法是配合html中锚点的使用，代码如下： 123[点击跳转到For Example3](#Test)&lt;a id=&quot;Test&quot;&gt;For Example3&lt;/a&gt; 其中id值只支持英文，id值和#后的值完全一致，不支持id值包含空格 当然，这样的代码并非按照MarkDown的思想直接将标题作为锚点的定位点，但是这样的锚点在vscode，github，csdn上都可以work！ 点击跳转到For Example3 … … … … … … … For Example3 … … … … …","path":"关于MD锚点的坑/","date":"08-04","excerpt":"","tags":[]},{"title":"Python正则表达式自查","text":"Python正则表达式自查 正则表达式是一种针对字符串表达“简洁”和“特征”思想的工具 正则表达式可以用来判断某字符串的特征归属 正则表达式语法由字符和操作符构成 1. Python正则表达式使用 2. Python正则表达式语法 3. Regex库方法 4. Match对象 5. 贪婪匹配和最小匹配 6. 实例 python正则表达式使用1234567891011121314import re# 不需要对转义符进行转义的原生字符串类型，直接表达一个正则表达式r&#x27;text&#x27; # 两种用法rst = re.search(r&#x27;[1‐9]\\d&#123;5&#125;&#x27;, &#x27;BIT 100081&#x27;)# compile方法可以使用原生字符串或字符串pat = re.compile(r&#x27;[1‐9]\\d&#123;5&#125;&#x27;)rst = pat.search(&#x27;BIT 100081&#x27;)if rst: print(rst.group(0)) python正则表达式语法1234567891011121314. 表示任何单个字符[] 对**单个字符**给出取值范围[^] 对**单个字符**给出排除范围* *前字符重复0~无限次+ +前字符重复1~无限次? ?前字符重复0~1次| 左右表达式任意一个&#123;m&#125; 对前一个字符重复m次&#123;m,n&#125; 对前一个字符重复[m,n]次，&#123;:3&#125;表示[0,3]次^ 匹配字符串开头$ 匹配字符串结尾() 分组标记，内部只能使用|操作符\\d 表示数字字符，等价于[0-9]\\w 表示单词字符，等价于[A-Za-z0-9_] regex库方法 re.search(pattern, string, flags&#x3D;0)在字符串中搜索匹配的第一个位置，返回match对象 re.match(pattern, string, flags&#x3D;0)从字符串开始位置起匹配正则表达式，返回match对象 re.findall(pattern, string, flags&#x3D;0)匹配所有的子串，返回列表 re.split(pattern, string, maxsplit&#x3D;0, flags&#x3D;0)将字符串按照正则表达式子串进行分割，返回列表 re.finditer(pattern, string, flags&#x3D;0)返回匹配的迭代类型，返回match对象 re.sub(pattern, repl, string, count&#x3D;0, flags&#x3D;0)在字符串中替换所有匹配子串，返回字符串 参数含义： 参数 含义 pattern 正则表达式的字符串或原生字符串表示 string 待匹配字符串 flags 正则表达式使用时的控制标记 maxsplit 最大分割数，剩余部分作为最后一个元素输出 repl 替换匹配字符串的字符串 count 最大替换次数 控制标记含义： 控制标记 含义 re.I re.IGNORECASE 忽略正则表达式的大小写，[A‐Z]能够匹配小写字符 re.M re.MULTILINE 正则表达式中的^操作符能够将给定字符串的每行当作匹配开始 re.S re.DOTALL 正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符 match对象Match对象是一次匹配的结果，包含匹配的所有信息 .string 待匹配的字符串 .re 用于匹配的正则表达式 .pos 搜索的开始位置 .endpos 搜索的结束位置 .group(0) 匹配的子串 .start() 匹配子串在字符串中的开始位置 .end() 匹配子串在字符串中的结束位置 .span() 返回(.start(), .end()) 贪婪匹配和最小匹配对于同一个匹配子串开始位置，可能存在多个结束位置满足匹配： Re库默认采用贪婪匹配，即输出匹配最长的子串 通过在操作符后增加?变成最小匹配 *? 前一个字符重复0~无限次，最小匹配 +? 前一个字符重复1~无限次，最小匹配 ?? 前一个字符重复0~1次，最小匹配 {m,n}? 前一个字符重复[m,n]次，最小匹配 实例12345[\\u4e00‐\\u9fa5] 匹配中文字符^[A‐Za‐z]+$ 由26个字母组成的字符串^‐?\\d+$ 整数^[0‐9]*[1‐9][0‐9]*$ 正整数(([1‐9]?\\d|1\\d&#123;2&#125;|2[0‐4]\\d|25[0‐5]).)&#123;3&#125;([1‐9]?\\d|1\\d&#123;2&#125;|2[0‐4]\\d|25[0‐5]) 匹配IPv4地址","path":"Python正则表达式自查/","date":"08-03","excerpt":"","tags":[]},{"title":"爬虫学习记录","text":"爬虫学习记录 使用python第三方库requests库进行网页爬取 get方法 post方法 Fn12利用浏览器抓包工具分析 User-Agent Content-Type 获取网页真正请求的url 查看网页提交请求的方法 Ajax数据分析 Asynchronous JavaScript and XML（异步的 JavaScript 和 XML） 页面局部刷新 利用爬虫爬取数据的第一步是分析页面 XML XML Extensiable Markup Language 可扩展标记语言 html -&gt; xhtml -&gt; xml Ajax基于XML实现 json模块(JavsScript Object Notation)一种信息标记格式规范 requests库的Response对象的json()方法返回一个json对象，类型为字典 json模块提供了4个方便的方法编码和解析json数据 json.dumps() -&gt; 将Python数据转换为json格式数据 jsom.loads() -&gt; 将json编码的字符串转换为一个Python数据结构 json.dump() -&gt; 用于文件 json.load() -&gt; 用于文件 json数据格式化工具 Python正则表达式 进行数据解析从而实现聚焦爬虫 bs4库 bs4是Python独有的一种解析数据方式 基于其它语言的爬虫不支持bs4 xpath库 xpath是通用的用于解析HTML数据的库 xpath的语法简洁，但是容易出现返回空列表问题 xpath尽量使用相对地址 验证码和打码 实现简单的模拟登录 base64数据编码 用于将数据转换为可见字符字符串 ip代理 解决封ip的反爬措施，但是https的免费代理不好找 Python线程池和进程池实现异步爬虫 selenium模块 对应浏览器的驱动放在python安装目录下 selenium4.0以上版本在3.x版本上做出了很大改变，API变化比较大，而且不向下兼容 看起来chorme浏览器更加配合selenium模块，不过在selenium4.x版本中对其它浏览器也比较友好了，目前没有遇到过增强包什么的 看起来selenium4.x中无头浏览器的使用API统一了，不需要额外依赖，而且不向下兼容 动态加载数据在对一个url发起请求时，页面中的数据可能由动态加载得到，重新对该url提交请求在抓包工具中查看对应url数据包，进一步判断请求得到的数据是否包含页面中的某些数据 判断网页是否采用AJax利用爬虫爬取网页数据时，首先需要分析数据到底是在服务器端组成好发回给浏览器的？还是通过AJax请求异步发送的？ AJax请求数据包在XHR选项中 利用AJax时，网页变化时url往往不会发生变化 对于一个AJax请求，会额外返回一个X-Requested-With: XMLHttpRequest头部字段，一个普通的请求没有该字段","path":"爬虫学习记录/","date":"08-01","excerpt":"","tags":[]},{"title":"反反爬","text":"反反爬总结以下都是在实践中遇到的反爬机制和对应的反反爬策略，虽然有些还没有解决，一点点总结下来终于明白了一句话 爬虫的顶端是js逆向和APP逆向 对于大部分简单的反爬机制而言，解决方案就藏在源代码中，主要是一个经验积累的过程 而对于复杂的反爬机制，可能不得不去读js代码，搞懂了js代码就可以逆向，搞不懂就只能被挡在外面 1. 百度安全验证 2. 某数反爬 3. 验证码 4. 隐藏域 5. 封ip 6. ajax携带随机参数 7. 伪url 8. http防盗链 9. selenium规避检测 百度安全验证通常修改request的请求头载体模拟时，使用浏览器的User-Agent字段： 1Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.134 Safari/537.36 Edg/103.0.1264.77 可能遭遇百度反爬拦截，自动跳转到百度安全验证 常用解决方案： 修改UA字段 增加更多头信息，例如Accept-Language 某数反爬在练习爬取化妆品生产许可信息管理服务平台时遇到HTTP状态码400，打开网络检查工具会直接跳转到源代码，一种说法是陷入无限debug中（每次返回的js代码不同），也无法添加断点，该站通过AJax请求更新页面时，提交的url携带两个随机生成的参数，无法通过后端的参数验证，暂时无法破解 一些某数逆向方法 使用fiddler进行请求拦截，进行调试 使用chrome的script监听器进行调试 本地搭建环境 验证码验证码，全称为Completely Automated Public Turing test to tell Computers and Humans Apart，即全自动区分计算机和人类的图灵测试（CAPTCHA）的缩写 顾名思义，验证码是一种反爬机制，识别验证码模拟登录一般有两个方法 肉眼识别 肉眼识别不是直接看浏览器图片，同样要爬取验证码图片到本地后再肉眼识别 借助第三方打码工具 云打码已经倒塌了 打码狗 超级鹰 经过两天的满口芬芳，终于是明白了模拟普通中有关验证码的种种： 首先，验证码图片的url一般是携带随机参数的，但是这个随机数和前端反爬没有关系，只是用来从后端库中随机选一张验证码图片（虽然url有点怪） 其次，在get登录页面时，登录页面会自动向这个url发请求get一张验证码图片回来在前端显示，同时后端会记录下发起get请求的这个东西（浏览器或者是爬虫）究竟请求到了哪张验证码图片，所以当我们登录时后端才能进行比对 所以事实上，前端登录页面显示出来的的验证码和后端关系其实并不大，后端并不care某一个东西究竟对登录页面get了多少次，后端care的是对验证码图片的url究竟get了多少次，只不过每次get登录页面时会自动get一下验证码图片来显示在前端，所以这两次get是有先后顺序的 这个先后顺序非常重要，因为后端保存的永远是最后一次get验证码的记录，当我们在浏览器访问一个登录页面，我们会看到一个验证码A，而如果我们再去另一个窗口访问一下验证码图片的url后，我们会看到另一个验证码B，事实上，验证码B才是当前正确登录需要的验证码 另外，大部分网站都会在登录后进行跳转，一般在response.headers[‘Location’]中保存，对应的状态码一般是302，303，程序并不需要特殊处理页面重定位，程序发起登录请求成功会自动进行页面的重定位直到取到最后登录成功的页面 最后一点也是卡了我半天的点，我们知道登录时会有set-cookie字段设置cookie，所以程序一般要用requests.session()生成session对象发登录请求，session会自动保存response对象的set-cookie，这没什么，但是！一定要在get验证码图片时就使用session，不要先随便get一下等到发登录请求时再用session，否则你会寄的很惨 附几个练习模拟登录的网站： 南京大学计算机系本科支撑教育平台 考试酷 古诗文网 隐藏域隐藏域是一种常见的反爬手段，post请求提交的参数是由input表单域发起的，因此所有的参数都应该在网页中有对应的input文本框，但是有一些参数没有对应的input文本框，这些参数被隐藏了，这类问题称为隐藏域问题。 隐藏域对于用户是不可见的，一些网站在发生请求时会将隐藏域中的值作为参数传递，如果提交时缺省这些参数可能无法通过参数匹配，常见的解决方法时在源码中搜索对应隐藏域，解析获取其value值 例如古诗词网在登陆时会用到_VIEWSTATE、_VIEWSTATEGENERATOR两个隐藏域 封ip在单位时间内某ip访问次数超过阈值，服务器会禁止该ip的访问，返回403或错误信息等 最简单的处理方法，随便找个代理网站，随便拿一个ip贴过来，proxies&#x3D;{‘’} 搭个代理池，用的时候从里面拿一个ip 大致思路是写个爬虫，把各个免费代理网站中的ip和端口号爬下来，测试可以用的存到本地，用的时候随便取一个 测试http代理，该网站会返回发起请求的ip，用于测试代理是否有效 代理ip时效性强弱和本机物理位置有关，所以写个爬虫随时更新代理池还是很有必要的 经过自己的测试，大部分免费的ip代理延时都很大，会出现连接错误，天下没有免费的午餐 另外没有必要纠结于某一个ip或某一个网站代理，只要不是爬虫的bug，出错的直接丢掉就好不必纠结错误原因 一些提供有效的免费代理的网站： 玛卡巴卡 唔西迪西 依古比古-该网站提供https代理 ajax携带随机参数在爬取梨视频时，其中的视频url是动态加载的，定位ajax请求却发现参数中携带mrd随机数 实际上，在ajax请求中携带的随机参数多数不用于反爬目的，是前端用于欺骗浏览器或服务器的 某些代理服务器会忽略no-cache之类的标识对响应结果强行缓存（例如IE），如果url不改变的话浏览器可能不会真正向服务器发送请求而调用自己的缓存 这样会导致后端的更新无法正确显示在前端，所以为保证每次请求都同服务器交互，前端会在url中携带一个随机数或一个时间戳，目的是使每次请求的url不同 对于爬虫的模拟，需要在js源代码中找到对应的参数，找到其随机数的生成方式，直接在爬虫中模拟即可 总之，ajax请求携带的随机参数本意不在反爬（也就是说后端不会对数值进行比对，只会对随机数格式进行比对），但正确找到随机数的生成方式确实会对爬虫造成阻碍 伪url同样在爬取梨视频时，ajax请求返回的json数据中包含了视频的url，但直接访问该url得到404 仔细观察，json数据中的url和最终前端标签中的url不完全一致，也就是说梨视频还进行了一个url的转换，json数据中的url是一个伪url 这个就比较容易了，找到转换的规律进行替换就可以 http防盗链http防盗链并不完全针对爬虫设计，但是确实起到了反爬的功能 http防盗链主要利用请求头部的Referer字段实现，Referer字段指示了请求的来源——即从哪里链接到了当前url 某些网站希望只能通过自己的网站链接到自己的资源时，会检查请求头的Referer字段，如果Referer字段不是自身网站的url则会拒绝访问并返回错误信息 通常在headers中增加Referer键值对，值为当前的网站url即可直接请求资源 selenium规避检测浏览器都带有一系列指纹特征，手动打开浏览器和通过selenium自动测试所携带的指纹特征并不一致，最典型的特征是当手动打开浏览器后，在控制台执行 window.navigator.webdriver会返回false，而通过selenium自动测试时会返回true 门户网站很容易根据这些指纹特征识别出selenium并且返回错误数据，常见的如登录验证错误 玛卡巴卡，这个网站用于检测是否来自selenium访问 检测到selenium后的WebDriver字段如下： 正常访问或成功反检测的WebDriver字段如下： 规避selenium检测的本质是修改指纹特征，避免门户网站识别，目前最有效的方法是在访问前先执行stealth.min.js脚本，可以做到规避检测 stealth.min.js源码 在爬虫程序中执行 123456789edge=webdriver.Edge()with open(&#x27;./stealth.min.js&#x27;) as f: js = f.read()edge.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123; &quot;source&quot;: js&#125;) 推荐-最完美方案！如何防止 Selenium 被检测出来","path":"反反爬/","date":"08-01","excerpt":"","tags":[]},{"title":"本地图片","text":"","path":"本地图片/","date":"07-30","excerpt":"","tags":[]},{"title":"MD语法自查","text":"MarkDown语法自查1. 标题 2. 列表 3. 字体 4. 锚点 5. 代码块 6. 图片和超链接 7. 表格 8. 引用 标题主标题副标题一级标题二级标题六级标题列表有序列表： 首先 首先 首首先 其其次 最最后 其次 最后 其次 最后 无序列表： 首先 首先 其次 最后 其次 最后 字体斜体 粗体 粗斜体 花裙子 删除线 删除线 锚点定位到这里 代码块1234int main()&#123; return 0;&#125; 12def func(age): return age int a=5+9; 图片和超链接 csdn社区 https://www.csdn.net/ 表格 姓名 年龄 职业 张三 20000 医生 李四 60 警察叔叔 引用张三说： 阿门阿前一棵葡萄树 葡萄树说：这是不对的","path":"MD语法自查/","date":"07-29","excerpt":"","tags":[]},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","path":"hello-world/","date":"07-17","excerpt":"","tags":[]}],"categories":[],"tags":[]}